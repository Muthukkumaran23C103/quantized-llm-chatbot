import asyncio
import os
import yaml
import time
from contextlib import asynccontextmanager
from typing import List, Optional
from fastapi import FastAPI, HTTPException, status, Depends, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.trustedhost import TrustedHostMiddleware
from fastapi.responses import PlainTextResponse
from pydantic import BaseModel
from dotenv import load_dotenv
import logging
import structlog

# Import Phase 4 modules
from src.auth.auth import auth_manager, get_current_user, require_admin_role, UserLogin, TokenResponse
from src.cache.redis_cache import cache, cached, CacheManager
from src.middleware.rate_limit import rate_limiter, RateLimitMiddleware, user_rate_limit_key
### #, get_prometheus_metrics

# Import existing modules
from src.database.vector_db import EnhancedVectorDatabase, SearchResult
from src.inference.ollama.client import OllamaClient, ChatMessage, ModelConfig
from src.nlp.processor import NLPProcessor, ProcessedText
# Add these imports to your existing main.py
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse
import os



# Load environment variables
load_dotenv()

# Configure logging
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.JSONRenderer() if os.getenv("ENVIRONMENT") == "production" else structlog.dev.ConsoleRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()

# Load model configuration
with open('config/models.yml', 'r') as f:
    config = yaml.safe_load(f)

# Global variables
ollama_client = None
db = None
nlp_processor = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    global ollama_client, db, nlp_processor
    
    logger.info("üöÄ Starting Phase 4: Production-Ready LLM Chatbot...")
    
    # Initialize Redis cache
    await cache.connect()
    await rate_limiter.connect()
    
    # Initialize NLP processor with caching
    try:
        nlp_processor = NLPProcessor()
        logger.info("‚úÖ NLP processor initialized")
    except Exception as e:
        logger.error("‚ùå Failed to initialize NLP processor", error=str(e))
        raise
    
    # Initialize enhanced database
    try:
        db = EnhancedVectorDatabase(db_path=os.getenv("DATABASE_PATH", "data/chatbot.db"))
        logger.info("‚úÖ Enhanced vector database initialized")
    except Exception as e:
        logger.error("‚ùå Failed to initialize database", error=str(e))
        raise
    
    # Initialize Ollama client
    try:
        ollama_client = OllamaClient()
        
        if not await ollama_client.health_check():
            logger.warning("‚ö†Ô∏è  Ollama server not responding")
        else:
            models = await ollama_client.list_models()
            logger.info("‚úÖ Ollama ready", model_count=len(models))
    except Exception as e:
        logger.error("‚ùå Failed to initialize Ollama client", error=str(e))
        raise
    
    yield
    
    # Shutdown
    if ollama_client:
        await ollama_client.close()
    if db:
        db.close()
    await cache.disconnect()
    logger.info("üîí Resources cleaned up")

# Create FastAPI app with production features
app = FastAPI(
    title="Production LLM Chatbot with Authentication & Caching",
    description="Phase 4: Production-ready system with auth, caching, rate limiting, and monitoring",
    version="0.4.0",
    lifespan=lifespan,
    docs_url="/docs" if os.getenv("ENVIRONMENT") != "production" else None,
    redoc_url="/redoc" if os.getenv("ENVIRONMENT") != "production" else None
)

app.mount("/static", StaticFiles(directory="src/frontend"), name="static")

@app.get("/ui", response_class=HTMLResponse)
async def web_interface():
    """Serve the modern web interface for students"""
    with open("src/frontend/web_ui.html", "r", encoding="utf-8") as f:
        return HTMLResponse(content=f.read())

@app.get("/gradio")
async def gradio_interface():
    """Redirect to Gradio interface"""
    return {"message": "Gradio interface available at port 7860", "url": "http://localhost:7860"}

# Add middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"] if os.getenv("ENVIRONMENT") == "development" else ["https://yourdomain.com"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.add_middleware(TrustedHostMiddleware, allowed_hosts=["*"])
# # app.add_middleware(RequestLoggingMiddleware)

# Add rate limiting middleware with different limits per endpoint
general_rate_limit = RateLimitMiddleware(calls=100, period=60)
api_rate_limit = RateLimitMiddleware(calls=50, period=60, key_func=user_rate_limit_key)

# Pydantic models for Phase 4
class HealthResponse(BaseModel):
    status: str
    version: str
    components: dict
    uptime_seconds: float
    cache_stats: dict

class DocumentUploadResponse(BaseModel):
    document_id: int
    filename: str
    status: str
    processing_time_ms: int
    analysis: dict
    cached: bool = False

class CachedChatResponse(BaseModel):
    user_message: str
    bot_response: str
    model_used: str
    session_id: Optional[str] = None
    response_time_ms: int
    context_documents: List[dict] = []
    processed_query: dict = {}
    from_cache: bool = False

# Helper functions
async def ensure_model_available(model_name: str) -> bool:
    """Ensure model is available with caching"""
    cache_key = f"model_available:{model_name}"
    
    # Check cache first
    is_available = await cache.get(cache_key)
    if is_available is not None:
        return is_available
    
    # Check actual availability
    models = await ollama_client.list_models()
    available_models = [m['name'] for m in models]
    available = model_name in available_models
    
    if not available:
        logger.info("üì• Downloading model", model=model_name)
        success = await ollama_client.pull_model(model_name)
        available = success
        if success:
            logger.info("‚úÖ Model downloaded", model=model_name)
    
    # Cache result for 1 hour
    await cache.set(cache_key, available, ttl=3600)
    return available

# Authentication routes
@app.post("/auth/login", response_model=TokenResponse)
async def login(user_credentials: UserLogin):
    """Authenticate user and return JWT tokens"""
    start_time = time.time()
    
    try:
        user = auth_manager.authenticate_user(user_credentials.username, user_credentials.password)
        if not user:
##            ##metrics.record_cache_operation("auth", "failed")
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Incorrect username or password"
            )
        
        # Create tokens
        access_token = auth_manager.create_access_token(data={"sub": user["username"], "role": user["role"]})
        refresh_token = auth_manager.create_refresh_token(data={"sub": user["username"]})
        
##        ##metrics.record_cache_operation("auth", "success")
        
        return TokenResponse(
            access_token=access_token,
            refresh_token=refresh_token,
            expires_in=1800  # 30 minutes
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error("Login error", error=str(e))
        raise HTTPException(status_code=500, detail="Authentication service error")

# Enhanced API Routes
@app.get("/", dependencies=[Depends(general_rate_limit)])
async def root():
    return {
        "message": "Production LLM Chatbot - Phase 4",
        "status": "running",
        "version": "0.4.0",
        "phase": "Production Enhancement",
        "features": [
            "JWT Authentication",
            "Redis Caching",
            "Rate Limiting",
            "Monitoring & Metrics",
            "Role-based Access Control",
            "Production Security"
        ]
    }

@app.get("/health", response_model=HealthResponse, dependencies=[Depends(general_rate_limit)])
async def enhanced_health_check():
    start_time = time.time()
    
    try:
        # Check all system components
        db_stats = db.get_database_stats()
        ollama_healthy = await ollama_client.health_check()
        cache_stats = await CacheManager.get_cache_stats()
##        app_info = ##metrics.get_application_info()
        
        return HealthResponse(
            status="healthy" if ollama_healthy else "partial",
            version="0.4.0",
            components={
                "database": "connected",
                "ollama": "connected" if ollama_healthy else "disconnected",
                "nlp_processor": "loaded",
                "redis_cache": cache_stats["status"],
                "rate_limiter": "active"
            },
            uptime_seconds=app_info["uptime_seconds"],
            cache_stats=cache_stats
        )
    except Exception as e:
        logger.error("Health check error", error=str(e))
        raise HTTPException(status_code=503, detail=f"Health check failed: {str(e)}")

@app.post("/documents", response_model=DocumentUploadResponse)
@cached(prefix="document_upload", ttl=3600)
async def upload_document_cached(
    document,
    current_user: str = Depends(get_current_user)
):
    """Upload document with caching and authentication"""
    import time
    start_time = time.time()
    
    try:
        # Process document with NLP
        processed_text = nlp_processor.preprocess_text(document.content)
        
        # Prepare processed data
        processed_data = {
            'cleaned_content': processed_text.cleaned_text,
            'tokens': processed_text.tokens,
            'entities': [entity for entity in processed_text.entities],
            'keywords': nlp_processor.extract_keywords(document.content),
            'word_count': len(document.content.split())
        }
        
        # Add document to database
        doc_id = db.add_document_with_embeddings(
            filename=document.filename,
            content=document.content,
            processed_data=processed_data,
            embedding=processed_text.embeddings,
            metadata=document.metadata
        )
        
        processing_time = int((time.time() - start_time) * 1000)
        
##        # Record metrics
##        ##metrics.record_database_operation("insert", "documents")
##        ##metrics.record_cache_operation("document_upload", "success")
        
        return DocumentUploadResponse(
            document_id=doc_id,
            filename=document.filename,
            status="processed",
            processing_time_ms=processing_time,
            analysis={
                "word_count": processed_data['word_count'],
                "tokens_extracted": len(processed_text.tokens),
                "entities_found": len(processed_text.entities),
                "keywords": processed_data['keywords'][:5],
                "embedding_dimensions": processed_text.embeddings.shape[0]
            }
        )
        
    except Exception as e:
        logger.error("Document upload error", error=str(e))
##        ##metrics.record_cache_operation("document_upload", "failed")
        raise HTTPException(status_code=500, detail=f"Document processing failed: {str(e)}")

@app.post("/chat", response_model=CachedChatResponse)
async def enhanced_chat_with_caching(
    request,
    current_user: str = Depends(get_current_user)
):
    """Enhanced chat with caching, authentication, and monitoring"""
    start_time = time.time()
    
    try:
        # Generate cache key based on message and context settings
        cache_key = f"chat:{hash(request.message)}:{request.use_context}:{request.max_context_docs}"
        
        # Check cache first
        cached_response = await cache.get(cache_key)
        if cached_response:
            logger.info("Cache hit for chat", user=current_user.username)
##            ##metrics.record_cache_operation("chat", "hit")
            cached_response["from_cache"] = True
            cached_response["response_time_ms"] = int((time.time() - start_time) * 1000)
            return CachedChatResponse(**cached_response)
        
        # Process user message
        processed_query = nlp_processor.preprocess_text(request.message)
        
        # Get relevant context if requested
        context_documents = []
        context_text = ""
        
        if request.use_context:
            search_results = db.semantic_search(
                query_embedding=processed_query.embeddings,
                limit=request.max_context_docs,
                distance_threshold=1.2
            )
            
            context_documents = [
                {
                    "filename": result.filename,
                    "similarity": round(result.similarity_score, 3),
                    "content_preview": result.content[:150] + "..."
                }
                for result in search_results
            ]
            
            if search_results:
                context_text = "\n\nRelevant context:\n" + "\n".join([
                    f"- {result.content[:200]}..."
                    for result in search_results[:2]
                ])
        
        # Build enhanced prompt
        enhanced_prompt = request.message
        if context_text:
            enhanced_prompt = f"{request.message}{context_text}\n\nPlease provide a response considering the context above."
        
        # Get model configuration
        model_config = get_model_config(request.model, "chat")
        
        # Ensure model is available
        if not await ensure_model_available(model_config.name):
            raise HTTPException(status_code=503, detail=f"Model {model_config.name} not available")
        
##        # Generate response with metrics
        inference_start = time.time()
        try:
            response = ollama_client.generate_sync(
                model=model_config.name,
                prompt=enhanced_prompt,
                config=model_config
            )
            inference_time = time.time() - inference_start
##            ##metrics.record_model_inference(model_config.name, inference_time, True)
        except Exception as e:
            inference_time = time.time() - inference_start
##            ##metrics.record_model_inference(model_config.name, inference_time, False)
            raise
        
        # Process bot response
        bot_processed = nlp_processor.preprocess_text(response)
        
        # Store chat history
        db.add_chat_with_embeddings(
            user_message=request.message,
            bot_response=response,
            user_embedding=processed_query.embeddings,
            bot_embedding=bot_processed.embeddings,
            session_id=request.session_id,
            context_documents=[result.document_id for result in search_results] if search_results else None
        )
        
        response_time = int((time.time() - start_time) * 1000)
        
        response_data = {
            "user_message": request.message,
            "bot_response": response,
            "model_used": model_config.name,
            "session_id": request.session_id,
            "response_time_ms": response_time,
            "context_documents": context_documents,
            "processed_query": {
                "entities": processed_query.entities,
                "keywords": nlp_processor.extract_keywords(request.message)
            },
            "from_cache": False
        }
        
        # Cache the response for future requests
        await cache.set(cache_key, response_data, ttl=1800)  # 30 minutes
        
##        ##metrics.record_cache_operation("chat", "miss")
##        ##metrics.record_database_operation("insert", "chat_history")
        
        return CachedChatResponse(**response_data)
        
    except Exception as e:
        logger.error("Enhanced chat error", error=str(e), user=current_user.username)
##        ##metrics.record_cache_operation("chat", "error")
        raise HTTPException(status_code=500, detail=f"Chat processing failed: {str(e)}")

# Admin routes
@app.get("/admin/stats")
async def admin_stats(admin_user = Depends(require_admin_role)):
    """Get comprehensive system statistics (admin only)"""
    try:
        db_stats = db.get_database_stats()
        cache_stats = await CacheManager.get_cache_stats()
##        app_info = ##metrics.get_application_info()
        
        return {
            "database": db_stats,
            "cache": cache_stats,
            "system": app_info,
            "models": await ollama_client.list_models()
        }
    except Exception as e:
        logger.error("Admin stats error", error=str(e))
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/admin/cache/clear")
async def clear_cache(admin_user = Depends(require_admin_role)):
    """Clear entire cache (admin only)"""
    try:
        cleared = await cache.clear_pattern("*")
        logger.info("Cache cleared by admin", cleared_keys=cleared, admin=admin_user.username)
        return {"status": "success", "cleared_keys": cleared}
    except Exception as e:
        logger.error("Cache clear error", error=str(e))
        raise HTTPException(status_code=500, detail=str(e))

# Metrics endpoint for Prometheus
##@app.get("/metrics", response_class=PlainTextResponse)
##async def prometheus_metrics():
##    """Prometheus metrics endpoint"""
##    return get_prometheus_metrics()

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        app,
        host="0.0.0.0", 
        port=8000,
        log_config=None  # Use our custom logging
    )

# Add this to your main.py after the existing endpoints

class ChatMessage(BaseModel):
    message: str
    use_context: bool = True
    max_context_docs: int = 3

class ChatResponse(BaseModel):
    bot_response: str
    response_time_ms: int = 0

@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(chat_message: ChatMessage):
    """Simple chat endpoint - returns echo for now"""
    
    # Simple demo response (you can enhance this later)
    response = f"ü§ñ Hello! You said: '{chat_message.message}'\n\n"
    response += "üìö This is a demo response from your AI Study Buddy!\n"
    response += "üí° To get full AI capabilities, connect Ollama or other LLM service.\n\n"
    response += "‚ú® Features available:\n"
    response += "‚Ä¢ üìÅ Document upload (coming soon)\n"
    response += "‚Ä¢ üîç Semantic search (coming soon)\n" 
    response += "‚Ä¢ üé® Beautiful student interface ‚úÖ\n"
    response += "‚Ä¢ üîê Authentication ‚úÖ"
    
    return ChatResponse(
        bot_response=response,
        response_time_ms=100
    )

@app.post("/documents")
async def upload_document():
    """Document upload endpoint placeholder"""
    return {"message": "üìÅ Document upload feature coming soon!"}

@app.post("/search")  
async def semantic_search():
    """Search endpoint placeholder"""
    return {"message": "üîç Semantic search feature coming soon!"}

