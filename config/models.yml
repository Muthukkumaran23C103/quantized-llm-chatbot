# Model Configurations for Different Use Cases

models:
  # Primary models for your hardware
  mistral_7b:
    name: "mistral:7b-instruct-q4_K_M"
    temperature: 0.7
    top_p: 0.9
    top_k: 40
    max_tokens: 2048
    description: "Best overall model for conversation and reasoning"
    vram_usage: "~4.4GB"
    
  tinyllama:
    name: "tinyllama:1.1b-chat-v1-q4_0"
    temperature: 0.8
    top_p: 0.9
    top_k: 40
    max_tokens: 1024
    description: "Ultra-fast responses for simple queries"
    vram_usage: "~637MB"
    
  phi2:
    name: "phi:2.7b-chat-v2-fp16"
    temperature: 0.7
    top_p: 0.9
    top_k: 40
    max_tokens: 1536
    description: "Balanced performance and efficiency"
    vram_usage: "~5.6GB"

# Use case specific configurations
use_cases:
  chat:
    preferred_model: "mistral_7b"
    fallback_model: "tinyllama"
    temperature: 0.7
    max_tokens: 2048
    
  summarization:
    preferred_model: "mistral_7b"
    fallback_model: "phi2"
    temperature: 0.3
    max_tokens: 1024
    
  search:
    preferred_model: "phi2"
    fallback_model: "tinyllama"
    temperature: 0.1
    max_tokens: 512
